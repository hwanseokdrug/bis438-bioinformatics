{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project2_ELAVL1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9hlk6898kI1o","colab_type":"text"},"source":["In this project, we will predict RBP-binding RNA sequences using a neural network model based on CNN (B. Alipanahi, et al. \"Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning\", Nature Biotechnology, 33, pages831–838 (2015)). \n","\n","1) Build CNN models for predicting the RBP-binding RNA sequences (ALKBH5 and ELAVL1).\n","\n","2) Train your model using the training dataset (and optimize your model using validation set).\n","\n","3) Report the ROC-AUC score of the test dataset using the trained model.\n","\n","4) Use google colab (or whatever you want)\n","\n","5) Upload  jupyter notebook (\".ipynb\" file) or link to your google colab code \n","\n","Due data: Oct/11th 23:59pm\n","\n","Grading criteria (10pt total):\n","- It runs anyway... --> 5pt\n","- Properly optimize the models --> 2pt\n","- Good performance --> 2pt\n","- Nice and proper comments --> 1pt\n","- It's a beautiful code! --> +3pt \n","\n","Project #2에서는 RBP-binding RNA sequence를 예측하는 것인데, 이를 확장해서 binding motif도 역시 구하는 extended project를 수행함\n","\n","프로젝트 수행 수준에 따라, 자네의 최종 점수는 중간고사의 class 평균점수 + alpha (물론 여기서 alpha는 negative number도 가능하다)"]},{"cell_type":"markdown","metadata":{"id":"wdSLxFhW1CDa","colab_type":"text"},"source":["# Prediction of ELAVL1 binding sequences\n","In this notebook, we **predict ELAVL1 binding RNA sequences with a CNN model.**\n","\n","Then as an extention, we also **analyze learned CNN filters to find local binding motif sequences.**\n","\n","To effectively detect binding motifs of different lengths, we used an adapted version of **inception module** used for GoogleNet in image classification task.\n","\n","The notebook is written as follows:\n","\n","**Basic project**\n","\n","- Module / data import\n","\n","- Train / validation set split\n","\n","- CNN architecture design\n","\n","- Hyperparameter setting & model optimization\n","\n","- Model evaluation on test dataset with ROC-AUC score\n","\n","**Extended project**\n","\n","- **Analysis of learned filters for local RNA sequence motifs**\n","\n","References:\n","\n","[1] B. Alipanahi, et al. \"Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning\", Nature Biotechnology, 33, pages831–838 (2015)\n","\n","[2] Szegedy et al., \"Going Deeper with Convolutions\", CVPR (2015)\n","\n","[3] Chung et al., \"Prediction of binding property of RNA-binding proteins using multi-sized filters and multi-model deep convolutional network\", PLOS ONE\n","\n","[4] Zhang et al., \"A deep learning framework for modeling structural\n","features of RNA-binding protein targets\", Nucleic Acids Research"]},{"cell_type":"markdown","metadata":{"id":"zaF-I2p91Kd2","colab_type":"text"},"source":["#Module / data import"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"I8BpnrYPieOm","colab":{}},"source":["#### project2_pseudo_code.py\n","\n","# import\n","import os\n","import numpy as np\n","import random\n","\n","import keras\n","import tensorflow as tf\n","###################\n","import tensorflow.keras as keras\n","from keras import Model\n","from keras.backend import one_hot\n","from keras.layers import Activation, Dense, Dropout, Input, BatchNormalization\n","from keras.layers import Conv1D, Concatenate, MaxPooling1D, AveragePooling1D, Flatten\n","from keras.regularizers import l2\n","from keras.optimizers import adam\n","from keras.utils import np_utils\n","###################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dFHGxvXqieX8","colab":{}},"source":["# import my google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ngC3R369iejR","colab":{}},"source":["# read fasta files  \n","fasta_file_path='gdrive/My Drive/data sample/'\n","rbp_name = 'ELAVL1'\n","print('list of files', os.listdir(fasta_file_path))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSIy9ZlWirzJ","colab_type":"code","colab":{}},"source":["# loading training set\n","train_fasta = list()\n","train_label = list()\n","for single_file in [x for x in os.listdir(fasta_file_path) if rbp_name in x and 'train' in x]:\n","  print('Processing file...', single_file)\n","  with open(fasta_file_path + single_file) as f:\n","    for line in f.readlines():\n","      # get fasta sequence\n","      if '>' in line:\n","        continue\n","      else:\n","        train_fasta.append(line.strip())\n","      # get positive negative label\n","      if 'positives' in single_file:\n","        train_label.append(1)\n","      else:\n","        train_label.append(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZmdxQGXhittI","colab_type":"code","colab":{}},"source":["# convert sequence file to one-hot encoding representation\n","# function: fasta to onehot representation\n","def convert2onehot(sequence_list):\n","  map = {\n","      'A':[1,0,0,0],\n","      'U':[0,1,0,0],\n","      'T':[0,1,0,0],\n","      'G':[0,0,1,0],\n","      'C':[0,0,0,1]\n","  }\n","  \n","  onehot = []\n","  for single_sequence in sequence_list:\n","    single_onehot = []\n","    for x in single_sequence:\n","      single_onehot.append(map[x.upper()])\n","    onehot.append(single_onehot)\n","    \n","  return np.asarray(onehot, dtype=np.float32)\n","\n","data_input = convert2onehot(train_fasta)\n","data_label = keras.utils.to_categorical(train_label, 2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmgzwvqo1QY_","colab_type":"text"},"source":["#Train / validation set split\n","\n","80% training set / 20% validation set"]},{"cell_type":"code","metadata":{"id":"PqxEHXkNuVOC","colab_type":"code","colab":{}},"source":["# Install sklearn package\n","!pip install imblearn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uy7IW3Gtiwx3","colab_type":"code","colab":{}},"source":["# split training set into training set and validation set\n","# random shuffling of training data\n","###################\n","# random shuffle indices for training data\n","arr = np.arange(data_input.shape[0])\n","np.random.shuffle(arr)\n","\n","# apply same shuffling to data and label\n","data_input = data_input[arr,:,:]\n","data_label = data_label[arr,:]\n","\n","# split training and validation dataset from raw dataset (9:1)\n","from sklearn.model_selection import train_test_split\n","train_input, validation_input, train_label, validation_label = train_test_split(data_input, data_label, test_size=0.1, random_state=1)\n","###################\n","print('Dataset preparation done... train_input, train_label, validation_input, validation_label')\n","print('Size of each set...', train_input.shape, train_label.shape, validation_input.shape, validation_label.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwBdJdNa1S4o","colab_type":"text"},"source":["#CNN architecture design\n","\n","**Design purpose: Build a model that predicts from & learns various lengths of sequence motifs.**\n","\n","Inception module from GoogleNet does this (concatenation of different sized filters, [2]).\n","\n","![inception_module](https://d2mxuefqeaa7sj.cloudfront.net/s_8C760A111A4204FB24FFC30E04E069BD755C4EEFD62ACBA4B54BBA2A78E13E8C_1490879611424_inception_module.png)\n","\n","We can adapt this architecture to sequence model simply by changing 2d filters to 1d.\n","\n","At the same time, we want to analyze learned filters in 1st inception module to look for RBP binding sequence motifs.\n","\n","From the requirements, we designed the model as follows:\n","\n","- Use simplified inception modules without 1-convolution sublayers\n","\n","- Instead of small (1-, 3-, 5-) convolutional filters, use large ones (8-, 16-, 32-) for later analysis of filters in the first inception module [3]\n","\n","- 3 inception modules -> Input size reduction by max-pooling -> FC layers for classification\n","\n","#Optimization / regularization techniques\n","\n","Below are some additional techniques we applied for optimization and regularization:\n","\n","- Adam optimizer\n","\n","- L2 weight regularization\n","\n","- Dropout before prediction layer\n","\n","- Early stopping\n","\n","#Model training curve\n","\n","![training_curve](https://drive.google.com/uc?export=download&id=1xgGiz_9_ECWh5_p8EaiL_V-wRw2jEeJN)"]},{"cell_type":"code","metadata":{"id":"1gbEyuDeSdkK","colab_type":"code","colab":{}},"source":["train_mode = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uZHppmRizSu","colab_type":"code","colab":{}},"source":["# model building\n","###################\n","# hyperparameters\n","lr = 3e-4\n","reg = 1e-2\n","p = 1/2\n","\n","# design purpose: build a model that predicts from / learns various lengths of sequence motifs.\n","# GoogleNet inception module-based CNN architecture\n","# use modified inception module instead of plain convolutional layers\n","\n","# inception module: detects motifs with different kernel sizes\n","# concatenation of activation maps from filters of variying lengths\n","\n","# use 1d convolution layers, starting from 4-channel input initially\n","inputs = Input(shape=(200,4,))\n","\n","# inception module 1\n","x1 = Conv1D(2, 8, strides=1, padding='same', data_format = 'channels_last')(inputs)\n","x2 = Conv1D(2, 16, strides=1, padding='same', data_format = 'channels_last')(inputs)\n","x3 = Conv1D(2, 32, strides=1, padding='same', data_format = 'channels_last')(inputs)\n","# concatenate motif scans along channel dimension\n","x = Concatenate(axis=2)([x1, x2, x3])\n","# ReLU activation\n","x = Activation('relu')(x)\n","# max-pooling\n","x = MaxPooling1D(pool_size=2, padding='valid', data_format='channels_last')(x)\n","\n","# inception module 2\n","x1 = Conv1D(2, 8, strides=1, padding='same', data_format = 'channels_last')(x)\n","x2 = Conv1D(2, 16, strides=1, padding='same', data_format = 'channels_last')(x)\n","x3 = Conv1D(2, 32, strides=1, padding='same', data_format = 'channels_last')(x)\n","# concatenate motif scans along channel dimension\n","x = Concatenate(axis=2)([x1, x2, x3])\n","# ReLU activation\n","x = Activation('relu')(x)\n","# max-pooling\n","x = MaxPooling1D(pool_size=2, padding='valid', data_format='channels_last')(x)\n","\n","# inception module 3\n","x1 = Conv1D(2, 8, strides=1, padding='same', data_format = 'channels_last')(x)\n","x2 = Conv1D(2, 16, strides=1, padding='same', data_format = 'channels_last')(x)\n","x3 = Conv1D(2, 32, strides=1, padding='same', data_format = 'channels_last')(x)\n","# concatenate motif scans along channel dimension\n","x = Concatenate(axis=2)([x1, x2, x3])\n","# ReLU activation\n","x = Activation('relu')(x)\n","# max-pooling\n","x = MaxPooling1D(pool_size=2, padding='valid', data_format='channels_last')(x)\n","\n","# reducing input size with 1-convolution and max pooling\n","x = Conv1D(6, 1, strides=1, data_format='channels_last')(x)\n","x = MaxPooling1D(pool_size=5, padding='valid', data_format='channels_last')(x)\n","x = Activation('relu')(x)\n","x = Conv1D(6, 1, strides=1, data_format='channels_last')(x)\n","x = MaxPooling1D(pool_size=5, padding='valid', data_format='channels_last')(x)\n","x = Activation('relu')(x)\n","\n","# flattening before FC layers\n","x = Flatten()(x)\n","# plain FC layers\n","x = Dense(64, kernel_regularizer=l2(reg))(x)\n","x = Activation('relu')(x)\n","x = Dropout(rate=p)(x)\n","# prediction layer\n","predictions = Dense(2, activation='softmax')(x)\n","\n","# model compilation\n","model = Model(inputs=inputs, outputs=predictions)\n","model.compile(optimizer=adam(lr=lr,decay=1e-5),\n","                 loss='categorical_crossentropy',\n","                 metrics=['accuracy'])\n","model.summary() # print summary of model\n","###################\n","\n","# model training\n","###################\n","if train_mode:\n","  # if training mode, train\n","  # early stopping with validation dataset\n","  from keras.callbacks import EarlyStopping\n","  from keras.callbacks import ModelCheckpoint\n","\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","  mc = ModelCheckpoint(fasta_file_path + 'ELAVL1_best.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","  # training the model using training and validation set\n","  epochs = 50\n","  # train AR-classification task\n","  hist = model.fit(train_input, train_label,\n","                   validation_data=(validation_input, validation_label),\n","                   batch_size=8, epochs=epochs, verbose=0, shuffle=True, callbacks=[es, mc])\n","  \n","  # visualize training curve\n","  import matplotlib.pyplot as plt\n","  fig, axs = plt.subplots(1,2, constrained_layout=True)\n","\n","  axs[0].plot(hist.history['loss'], label='train loss')\n","  axs[0].plot(hist.history['val_loss'], label='val loss')\n","  axs[0].legend(loc='upper left')\n","  axs[0].set_title('loss')\n","\n","  axs[1].plot(hist.history['acc'], label='train acc')\n","  axs[1].plot(hist.history['val_acc'], label='val acc')\n","  axs[1].legend(loc='upper left')\n","  axs[1].set_title('acc')\n","\n","else:\n","  # if not training mode, load pre-trained weights\n","  model.load_weights(fasta_file_path + 'ELAVL1_best.h5')\n","  print(\"Model weights loaded\")\n","###################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L0TIEitR1WJf","colab_type":"text"},"source":["#Model evaluation on test dataset with ROC-AUC score\n","\n","With trained model, we report the performance on test set as follows:\n","\n","Classification accuracy: 93%\n","\n","AUC-ROC score: 0.97\n","\n","![roc_curve](https://drive.google.com/uc?export=download&id=1igHW3s2RsPE3m_y0ZA2ILKZ_RxJj1jON)"]},{"cell_type":"code","metadata":{"id":"rKyMS4CVi1gA","colab_type":"code","colab":{}},"source":["# evaluation: calculate AUC score for the test set\n","# loading test sets \n","###################\n","# loading training set\n","test_fasta = list()\n","test_label = list()\n","for single_file in [x for x in os.listdir(fasta_file_path) if rbp_name in x and 'test' in x]:\n","  print('Processing file...', single_file)\n","  with open(fasta_file_path + single_file) as f:\n","    for line in f.readlines():\n","      # get fasta sequence\n","      if '>' in line:\n","        continue\n","      else:\n","        test_fasta.append(line.strip())\n","      # get positive negative label\n","      if 'positives' in single_file:\n","        test_label.append(1)\n","      else:\n","        test_label.append(0)\n","###################\n","\n","test_input = convert2onehot(test_fasta)\n","test_label = keras.utils.to_categorical(test_label, 2)\n","print('TEST set prepared...', test_input.shape, test_label.shape)\n","\n","# calculate AUC score for test sets\n","###################\n","# evaluate the prediction model using test dataset.\n","from sklearn.metrics import classification_report, roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","test_loss, test_acc = model.evaluate(test_input, test_label)\n","test_pred = model.predict(test_input)\n","\n","# 0: negative, 1: positive\n","target_names = ['0','1']\n","print(\"classification report\")\n","print(classification_report(test_label[:,1]==1, test_pred[:,1]>0.5, target_names=target_names))\n","\n","# compute and draw ROC curve\n","fpr, tpr, thresholds = roc_curve(test_label[:,1], test_pred[:,1])\n","roc_auc = auc(fpr, tpr)\n","\n","fig = plt.figure()\n","plt.plot(fpr, tpr, \"b\")\n","plt.plot([0, 0], [0, 1], \"k--\")\n","plt.plot([0, 1], [1, 1], \"k--\")\n","plt.plot([0, 1], [0, 1], \"k--\")\n","plt.title(\"AUC = %0.2f\" % roc_auc)\n","plt.xlabel(\"FP rate (1 - specificity)\")\n","plt.ylabel(\"TP rate (sensitivity)\")\n","###################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9zydjSN1cXw","colab_type":"text"},"source":["#Extended project: Analysis of learned filters for local RNA sequence motifs\n","\n","In this section, we  \n","(1) analyze learned filters of various lengths in our inception-like CNN,  \n","(2) try to find RNA sequence motifs important for RBP binding and  \n","(3) assess their biological plausibility.\n","\n","Convolution filters, or motif detectors learned from RBP data, are shown below as 4 x length colormaps (each columns were mean-subtracted before visualization).\n","\n","We can see that, for example, the first 8-length filter detects U-rich local region and the first 16-length filter specifically detects \"CUCCUCCAUGG...\".\n","\n","![f8](https://drive.google.com/uc?export=download&id=1fnsSsxMJ0UATQquwDsx1zaQlN0RY3UST)\n","\n","![f16](https://drive.google.com/uc?export=download&id=1TXNbLotXTMkSEFLTOYWjTQ9ezySponHH)\n","\n","![f32](https://drive.google.com/uc?export=download&id=1uKDrw86-gsX_IrnYZOVb_ZFPcJZiV2lz)"]},{"cell_type":"code","metadata":{"id":"FrP5a_Oh1eqH","colab_type":"code","colab":{}},"source":["# load convolution filters from learned model\n","conv1 = model.layers[1].get_weights()\n","conv2 = model.layers[2].get_weights()\n","conv3 = model.layers[3].get_weights()\n","# load w and b\n","w1 = conv1[0]; b1 = conv1[1]\n","w2 = conv2[0]; b2 = conv2[1]\n","w3 = conv3[0]; b3 = conv3[1]\n","\n","# function for visualization of learned filters\n","def vis_filter(length, w):\n","  fig = plt.figure()\n","  plt.title(\"filter length = \"+str(length))\n","  plt.axis('off')\n","  for ii in range(2):\n","    ax = fig.add_subplot(2,1,ii+1)\n","    temp_img = w[:,:,ii].transpose() # filter image\n","    ax.imshow(temp_img-np.mean(temp_img,0),cmap='bwr') # mean subtraction for each position\n","    ax.set_xticks([],[])\n","    plt.yticks((0,1,2,3), ('A','U','C','G'))\n","\n","# plot w of 8-length filter as a heatmap\n","vis_filter(8, w1)\n","# plot w of 16-length filter as a heatmap\n","vis_filter(16, w2)\n","# plot w of 32-length filter as a heatmap\n","vis_filter(32, w3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mt_hwVdm1hnf","colab_type":"text"},"source":["# (source code for sequence logo)\n","modified from: https://github.com/azofeifa/PWM_logo/tree/master/src\n","\n"]},{"cell_type":"code","metadata":{"id":"GVCwRfuQ1frP","colab_type":"code","colab":{}},"source":["from math import log\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","\n","def despine(ax,right=True, top=True, left=False,bottom=False):\n","    if left:\n","        ax.spines['left'].set_visible(False)\n","    if bottom:\n","        ax.spines['bottom'].set_visible(False)\n","    if right:\n","        ax.spines['right'].set_visible(False)\n","    if top:\n","        ax.spines['top'].set_visible(False)\n","\n","def draw_A(width, height, x, y, ax, color=\"blue\",lw=2.0,alpha=0.5):\n","    ax.plot([x, x+width*0.5], [y, y+height],lw=lw, color=color ,alpha=alpha)\n","    ax.plot([x+width*0.5, x+width], [y+height, y],lw=lw, color=color ,alpha=alpha)\n","    ax.plot([x+width*0.25,x+width*0.75], [height*0.5, height*0.5], color=color, lw=lw,alpha=alpha)\n","    return y+height\n","\n","def draw_U(width, height,x,y, ax, color=\"red\",lw=2.0,alpha=0.5):\n","    ax.plot([x, x], [y+height/2, y+height],lw=lw, color=color,alpha=alpha)\n","    ax.plot([x+width, x+width], [y+height/2, y+height],lw=lw, color=color,alpha=alpha)\n","    x += width/2\n","    y += height/2\n","    ts = np.linspace(math.pi, math.pi*2, 100)\n","    xs = [math.cos(t)*width/2+x for t in ts]\n","    ys = [math.sin(t)*height/2+y for t in ts]\n","    ax.plot(xs,ys,lw=lw, color=color,alpha=alpha)\n","    return y + height/2\n","\n","def draw_C(width, height,x,y, ax, color=\"green\",lw=2.0,alpha=0.5):\n","    x += width/2\n","    y += height/2\n","    ts = np.linspace(3.85*math.pi/2., math.pi/6., 100)\n","    xs = [math.cos(t)*width/2+x for t in ts]\n","    ys = [math.sin(t)*height/2+y for t in ts]\n","    ax.plot(xs,ys,lw=lw, color=color,alpha=alpha)\n","    return y + height/2\n","\n","def draw_G(width, height,x,y, ax, color=\"orange\",lw=2.0,alpha=0.5):\n","    x += width/2\n","    y += height/2\n","    ts = np.linspace(3.85*math.pi/2., math.pi/6., 100)\n","    xs = [math.cos(t)*width/2+x for t in ts]\n","    ys = [math.sin(t)*height/2+y for t in ts]\n","    ax.plot(xs,ys,lw=lw, color=color,alpha=alpha)\n","    ptl = x, y-height/2\n","    ptu = xs[0],ys[0]\n","    ax.plot([ptu[0],ptu[0]], [ptu[1], ptl[1]],lw=lw, color=color,alpha=alpha)\n","    ax.plot([ptu[0]-width*0.75/2,ptu[0]], [ptu[1], ptu[1]],lw=lw, color=color,alpha=alpha)\n","    return y + height/2\n","\n","def draw_seq_log(PSSM,title, AX=None,lw=1.0,alpha=1):\n","    ax = AX\n","    if AX is None:\n","        F = plt.figure(facecolor=\"white\")\n","        ax = plt.gca()\n","    ax.set_title(title, fontsize=10)\n","    A = np.array(PSSM)\n","    for i in range(A.shape[0]):\n","        A[i,:] /= sum(A[i,:])\n","        methods = (draw_A, draw_U, draw_G, draw_C)\n","        x = i\n","        width = 0.9\n","        y = 0.0\n","        I = 2+sum([A[i,l]*math.log(A[i,l],2)  for l in range(4)])\n","        for j in range(4):\n","            d = methods[j]\n","            h = I*A[i,j]\n","            #print(h)\n","            y = d(width, h, x, y, ax, lw=lw, alpha=alpha)\n","    ax.set_xlim(0,A.shape[0])\n","    ax.set_xticks([])\n","    ax.set_xlabel(\"Position\",fontsize=10)\n","    ax.set_ylabel(\"bits\",fontsize=10)\n","    despine(ax,bottom=True)\n","    ax.yaxis.grid(True)\n","    ax.set_ylim(0,2)\n","    if AX is None:\n","        plt.show()\n","\n","class logo:\n","    def __init__(self, matrix, name=\"\"):\n","        self.m      = matrix\n","        self.name   = name\n","    def draw(self ,ax=None):\n","        draw_seq_log(self.m,self.name, AX=ax)\n","\n","def main():\n","    A=[[ 0.356611,0.085184,0.455087,0.103118],\n","       [ 0.217352,0.033832,0.616781,0.132036],\n","       [ 0.411544,0.053398,0.486295,0.048762],\n","       [ 0.026776,0.902577,0.020027,0.050619],\n","       [ 0.773107,0.016504,0.055813,0.154576],\n","       [ 0.121956,0.027309,0.046496,0.804239],\n","       [ 0.010707,0.015609,0.947804,0.025881],\n","       [ 0.031773,0.310079,0.022036,0.636112],\n","       [ 0.055142,0.622140,0.063964,0.258755],\n","       [ 0.176130,0.035488,0.131170,0.657211],\n","       [ 0.095022,0.092511,0.793152,0.019314],\n","       [ 0.190134,0.058570,0.593589,0.157707],\n","       [ 0.456692,0.015034,0.473979,0.054296],\n","       [ 0.008153,0.976192,0.001984,0.013671],\n","       [ 0.767356,0.030153,0.037623,0.164869],\n","       [ 0.170081,0.032247,0.065448,0.732224],\n","       [ 0.036761,0.017177,0.926087,0.019975],\n","       [ 0.059032,0.416386,0.015859,0.508723],\n","       [ 0.114088,0.584113,0.060812,0.240987],\n","       [ 0.224960,0.355487,0.071233,0.348320]]\n","    #L        = logo(A, name=\"P53\")\n","    #F        = plt.figure(figsize=(12,10),facecolor=\"white\")\n","    #ax       = plt.gca()\n","    #L.draw(ax=ax)\n","    #plt.show()\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdajwNpH1jw6","colab_type":"text"},"source":["#Extended project: Binding motifs & sequence logos\n","\n","To visualize the patterns learned by each 1st layer filters, we generate a PPM (position probability matrix) derived from the filter's response to actual sequences[1].\n","\n","1. All positive sequences from training and test set are fed through the convolutional and rectifying (ReLU) operations of the model.\n","\n","2. Then, all the sequences that passed the ReLU threshold (activation>0 for one or more positions in activation map) are aligned.\n","\n","3. From the aligned sequences (assembly), a PPM is computed by counting the occurrences of bases in each position, which is then transformed to a sequence logo.\n","\n","As a result, we get 1 sequence motif per 1 convolutional filter.\n","\n","Below are sequence logos of learned filters:\n","\n","**Filter length 8**\n","\n","8-1\n","\n","![1](https://drive.google.com/uc?export=download&id=11_VIrpsoGi8LXwViwcdMgf7Hy0NaXOn6)\n","\n","8-2\n","\n","![2](https://drive.google.com/uc?export=download&id=1N52MgzFdRzbKx4Eee9q5osiAWf4ewXSK)\n","\n","**Filter length 16**\n","\n","16-1\n","\n","![3](https://drive.google.com/uc?export=download&id=1HVqxLAKE9_uJPE15ENLhvzGNZXK6oR4I)\n","\n","16-2\n","\n","![4](https://drive.google.com/uc?export=download&id=1x21aEjqTAhXar1cFfeppG3WIwbjewqIo)\n","\n","**Filter length 32**\n","\n","32-1\n","\n","![5](https://drive.google.com/uc?export=download&id=1CEuQhbOtgghPifaBNDOUojuCjPDDJYK7)\n","\n","32-2\n","\n","![6](https://drive.google.com/uc?export=download&id=1NjuAwQbJy4Jfx1f8psVQApu3mDONAttN)\n","\n","Let's compare the sequences to RBP-binding motifs of ELAVL1 known in literature.\n","\n","**Known motif**\n","\n","CISBP-RNA database\n","\n","![literature](https://drive.google.com/uc?export=download&id=1d0ckS1s8Tu9Nowgo39NXCDpygQPUwoBA)\n","\n","HITS-CLIP database\n","\n","![literature2](https://drive.google.com/uc?export=download&id=1eWzhflUmpcPfSihwTvaM-ghwLHyLqpdL)\n","\n","As seen from sequences above, the CNN filters learn to detect short sequences with frequent occurence of U / AU neucleotides (8-1, 8-2, 16-1, 16-2, 32-1), which is in fact similar to RBP-binding motif of ELAVL1 known from experiment.\n","\n","This implies that filters of CNN optimized for RBP-binding prediction is able to capture biologically important sequence motifs in RBP-binding process.\n","\n","**Biological meaning of this binding motif is discussed later in project2_ALKBH5.ipynb!**"]},{"cell_type":"code","metadata":{"id":"iwfql_W1X32i","colab_type":"code","colab":{}},"source":["# gather positive RBP sequences\n","positive = np.reshape(np.asarray(np.where(test_label[:,1])),-1) # use only positive test cases\n","bind_input = test_input[positive,:,:]\n","positive = np.reshape(np.asarray(np.where(train_label[:,1])),-1) # use only positive train cases\n","bind_input = np.concatenate((bind_input,train_input[positive,:,:]),axis=0)\n","\n","# function for PPM computation and sequence logo generation\n","def gen_logo(length, w, b, logo_width):\n","  # generating sequence logos\n","  # n: filter length\n","  # w: filters as weight matrices\n","  # b: filter biases\n","  # logo_width: sequence logo width\n","  n = 2 # number of filters per each length\n","  n_test = bind_input.shape[0] # number of test data\n","  thr = 0\n","  #thr = np.sum(np.max(w,1),0)*0.7\n","\n","  w_2d = np.reshape(w,(-1,n))\n","  seq = np.zeros((n_test,length,4,n)) # subsequences yielding max response for a filter\n","  keep = np.zeros((n_test,n)) # sequences to keep / discard\n","\n","  # Align sequences for each filters\n","  for ii in range(n_test):\n","    img = bind_input[ii,:,:] # an input image, 200*4\n","    s = np.zeros((200-length,n)) # sliding activation map, slide*n\n","    for jj in range(200-length): # sliding convolution, 'valid'\n","      img_1d = np.reshape(img[jj:jj+length,:],(1,-1)) # 1*24\n","      s[jj,:] = np.matmul(img_1d,w_2d) # convolution operation\n","      s[jj,:] += b # bias addition\n","    \n","    max_s = np.max(s,0) # compute maximum activation of each filters\n","    keep[ii,:] = max_s>thr # determine exclusion from assembly\n","    # select max scoring subsequence for each filter\n","    max_t = np.argmax(s,0)\n","    for ff in range(n): seq[ii,:,:,ff] = img[max_t[ff]:max_t[ff]+length,:]\n","\n","  # Select positive-responding sequences and create PPM of each filter\n","  for ff in range(n):\n","    # compute PPM of each filter\n","    tmp = np.reshape(np.asarray(np.where(keep[:,ff])),-1) # sequences to exclude / include\n","    ppm = np.mean(seq[tmp,:,:,ff],0) # compute PPM from aligned assembly\n","    ppm[np.where(ppm==0)] += 1e-10 # for numerical stability\n","\n","    # generate sequence logo\n","    L = logo(ppm, name=str(ff+1))\n","    F = plt.figure(figsize=(logo_width,3),facecolor=\"white\")\n","    ax = plt.gca()\n","    L.draw(ax=ax)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hm7xpydo1mJI","colab_type":"code","colab":{}},"source":["gen_logo(length=8, w=w1, b=b1, logo_width = 5)\n","gen_logo(length=16, w=w2, b=b2, logo_width = 10)\n","gen_logo(length=32, w=w3, b=b3, logo_width = 15)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIKO1GdH7Yo6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}